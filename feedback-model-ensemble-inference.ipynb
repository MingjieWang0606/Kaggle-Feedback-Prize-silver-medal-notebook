{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33992a1b",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-03-06T07:23:16.027812Z",
     "iopub.status.busy": "2022-03-06T07:23:16.027425Z",
     "iopub.status.idle": "2022-03-06T07:23:16.082032Z",
     "shell.execute_reply": "2022-03-06T07:23:16.081319Z",
     "shell.execute_reply.started": "2022-03-06T07:23:16.027644Z"
    },
    "papermill": {
     "duration": 0.019147,
     "end_time": "2022-03-15T18:50:05.058596",
     "exception": false,
     "start_time": "2022-03-15T18:50:05.039449",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "I have repeatedly experienced submission errors in the past few days, such as \"Notebook Threw Exception\", \"Notebook Exceeded Allowed Compute\" and \"Notebook Timeout\". Now, I modified the inference code for fast and safe submission(one deberta-large model takes only 17 minute), it works fine now. Hope it can helps.\n",
    "\n",
    "Thanks for the following greate notebooks:\n",
    "\n",
    "1. https://www.kaggle.com/hengck23/1-birdformer-1-longformer-one-fold\n",
    "2. https://www.kaggle.com/abhishek/two-longformers-are-better-than-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00140051",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T18:50:05.125810Z",
     "iopub.status.busy": "2022-03-15T18:50:05.120747Z",
     "iopub.status.idle": "2022-03-15T18:50:05.165764Z",
     "shell.execute_reply": "2022-03-15T18:50:05.166863Z",
     "shell.execute_reply.started": "2022-03-15T18:43:47.727636Z"
    },
    "papermill": {
     "duration": 0.08693,
     "end_time": "2022-03-15T18:50:05.167256",
     "exception": false,
     "start_time": "2022-03-15T18:50:05.080326",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The following is necessary if you want to use the fast tokenizer for deberta v2 or v3\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "transformers_path = Path(\"/opt/conda/lib/python3.7/site-packages/transformers\")\n",
    "\n",
    "input_dir = Path(\"../input/deberta-v2-3-fast-tokenizer\")\n",
    "\n",
    "convert_file = input_dir / \"convert_slow_tokenizer.py\"\n",
    "conversion_path = transformers_path/convert_file.name\n",
    "\n",
    "if conversion_path.exists():\n",
    "    conversion_path.unlink()\n",
    "\n",
    "shutil.copy(convert_file, transformers_path)\n",
    "deberta_v2_path = transformers_path / \"models\" / \"deberta_v2\"\n",
    "\n",
    "for filename in ['tokenization_deberta_v2.py', 'tokenization_deberta_v2_fast.py', \"deberta__init__.py\"]:\n",
    "    if str(filename).startswith(\"deberta\"):\n",
    "        filepath = deberta_v2_path/str(filename).replace(\"deberta\", \"\")\n",
    "    else:\n",
    "        filepath = deberta_v2_path/filename\n",
    "    if filepath.exists():\n",
    "        filepath.unlink()\n",
    "\n",
    "    shutil.copy(input_dir/filename, filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9e007f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T18:50:05.217070Z",
     "iopub.status.busy": "2022-03-15T18:50:05.216174Z",
     "iopub.status.idle": "2022-03-15T18:50:07.658531Z",
     "shell.execute_reply": "2022-03-15T18:50:07.659049Z",
     "shell.execute_reply.started": "2022-03-15T18:43:47.750980Z"
    },
    "papermill": {
     "duration": 2.472087,
     "end_time": "2022-03-15T18:50:07.659261",
     "exception": false,
     "start_time": "2022-03-15T18:50:05.187174",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../input/tez-lib/\")\n",
    "\n",
    "import gc\n",
    "import numpy as np\n",
    "import glob\n",
    "import pandas as pd\n",
    "from timeit import default_timer as timer\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import torch\n",
    "from torch.nn.parallel.data_parallel import data_parallel\n",
    "\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import *\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer\n",
    "import torch.nn as nn\n",
    "import psutil\n",
    "import torch.cuda.amp as amp\n",
    "import os\n",
    "import tez\n",
    "\n",
    "\n",
    "\n",
    "is_amp   = True\n",
    "is_cuda  = True\n",
    "is_debug = False # False\n",
    "\n",
    "max_length = 1536 # 1536\n",
    "submit_dir = ''\n",
    "\n",
    "\n",
    "#helper\n",
    "def time_to_str(t, mode='min'):\n",
    "    if mode=='min':\n",
    "        t  = int(t)/60\n",
    "        hr = t//60\n",
    "        min = t%60\n",
    "        return '%2d hr %02d min'%(hr,min)\n",
    "    elif mode=='sec':\n",
    "        t   = int(t)\n",
    "        min = t//60\n",
    "        sec = t%60\n",
    "        return '%2d min %02d sec'%(min,sec)\n",
    "    else:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e25126e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T18:50:07.698016Z",
     "iopub.status.busy": "2022-03-15T18:50:07.697248Z",
     "iopub.status.idle": "2022-03-15T18:50:07.743072Z",
     "shell.execute_reply": "2022-03-15T18:50:07.742626Z",
     "shell.execute_reply.started": "2022-03-15T18:43:47.776326Z"
    },
    "papermill": {
     "duration": 0.072191,
     "end_time": "2022-03-15T18:50:07.743239",
     "exception": false,
     "start_time": "2022-03-15T18:50:07.671048",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(valid_id) 5\n",
      "df_text.shape (5, 2)\n",
      "             id                                               text\n",
      "0  D46BCB48440A  When people ask for advice,they sometimes talk...\n",
      "1  D72CB1C11673  Making choices in life can be very difficult. ...\n",
      "2  DF920E0A7337  Have you ever asked more than one person for h...\n",
      "3  0FB0700DAF44  During a group project, have you ever asked a ...\n",
      "4  18409261F5C2  80% of Americans believe seeking multiple opin...\n"
     ]
    }
   ],
   "source": [
    "#config \n",
    "\n",
    "discourse_marker_to_label = {\n",
    "    \"B-Lead\": 0,\n",
    "    \"I-Lead\": 1,\n",
    "    \"B-Position\": 2,\n",
    "    \"I-Position\": 3,\n",
    "    \"B-Evidence\": 4,\n",
    "    \"I-Evidence\": 5,\n",
    "    \"B-Claim\": 6,\n",
    "    \"I-Claim\": 7,\n",
    "    \"B-Concluding Statement\": 8,\n",
    "    \"I-Concluding Statement\": 9,\n",
    "    \"B-Counterclaim\": 10,\n",
    "    \"I-Counterclaim\": 11,\n",
    "    \"B-Rebuttal\": 12,\n",
    "    \"I-Rebuttal\": 13,\n",
    "    \"O\": 14,\n",
    "    \"PAD\": -100,\n",
    "}\n",
    "label_to_discourse_marker = {v: k for k, v in discourse_marker_to_label.items()}\n",
    "num_discourse_marker = 15 #len(label_to_discourse_marker)-1 #15\n",
    "\n",
    "length_threshold = {\n",
    "    'Lead'                : 5, # 5\n",
    "    'Position'            : 5,\n",
    "    'Claim'               : 3,\n",
    "    'Counterclaim'        : 6,\n",
    "    'Rebuttal'            : 3, # 4\n",
    "    'Evidence'            : 14,\n",
    "    'Concluding Statement': 5, # 5\n",
    "}\n",
    "probability_threshold = {\n",
    "    'Lead'                : 0.687,\n",
    "    'Position'            : 0.537,\n",
    "    'Claim'               : 0.537,\n",
    "    'Counterclaim'        : 0.535,\n",
    "    'Rebuttal'            : 0.537,\n",
    "    'Evidence'            : 0.637,\n",
    "    'Concluding Statement': 0.687,\n",
    "}\n",
    "\n",
    "\n",
    "if is_debug:\n",
    "    text_dir = '../input/feedback-prize-2021/train'\n",
    "    df = pd.read_csv('../input/feedbackfolds/train_folds.csv')\n",
    "    valid_df = df[:10000]\n",
    "    # valid_df = df[df['kfold'] == 0].reset_index(drop=True)\n",
    "    valid_id = valid_df['id'].unique()\n",
    "\n",
    "else:\n",
    "    text_dir = '../input/feedback-prize-2021/test'\n",
    "    valid_id = [ f.split('/')[-1][:-4] for f in glob.glob(text_dir+'/*.txt') ]\n",
    "    valid_id = sorted(valid_id)\n",
    "num_valid = len(valid_id)\n",
    "print('len(valid_id)',len(valid_id))\n",
    "\n",
    "df_text=[]\n",
    "for id in valid_id:\n",
    "    text_file = text_dir +'/%s.txt'%id\n",
    "    with open(text_file, 'r') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    text = text.replace(u'\\xa0', u' ')\n",
    "    text = text.rstrip()\n",
    "    text = text.lstrip()\n",
    "    df_text.append((id,text))\n",
    "df_text = pd.DataFrame(df_text, columns=['id','text'])\n",
    "df_text['text_len'] = df_text['text'].apply(lambda x: len(x))\n",
    "df_text = df_text.sort_values('text_len').reset_index(drop=True)\n",
    "del df_text['text_len']\n",
    "\n",
    "print('df_text.shape',df_text.shape)\n",
    "print(df_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f59ee512",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T18:50:07.791401Z",
     "iopub.status.busy": "2022-03-15T18:50:07.790509Z",
     "iopub.status.idle": "2022-03-15T18:50:07.792470Z",
     "shell.execute_reply": "2022-03-15T18:50:07.791953Z",
     "shell.execute_reply.started": "2022-03-15T18:43:47.800940Z"
    },
    "papermill": {
     "duration": 0.037808,
     "end_time": "2022-03-15T18:50:07.792598",
     "exception": false,
     "start_time": "2022-03-15T18:50:07.754790",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _prepare_test_data_helper(tokenizer, ids):\n",
    "    test_samples = []\n",
    "    for idx in ids:\n",
    "        text_file = text_dir +'/%s.txt' % idx\n",
    "        with open(text_file, 'r') as f:\n",
    "            text = f.read()\n",
    "\n",
    "        text = text.replace(u'\\xa0', u' ')\n",
    "        text = text.rstrip()\n",
    "        text = text.lstrip()\n",
    "\n",
    "        encoded_text = tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=False,\n",
    "            return_offsets_mapping=True,\n",
    "        )\n",
    "        input_ids = encoded_text[\"input_ids\"]\n",
    "        offset_mapping = encoded_text[\"offset_mapping\"]\n",
    "\n",
    "        sample = {\n",
    "            \"id\": idx,\n",
    "            \"input_ids\": input_ids,\n",
    "            \"text\": text,\n",
    "            \"offset_mapping\": offset_mapping,\n",
    "        }\n",
    "\n",
    "        test_samples.append(sample)\n",
    "    return test_samples\n",
    "\n",
    "\n",
    "def prepare_test_data(df, tokenizer):\n",
    "    test_samples = []\n",
    "    ids = df[\"id\"].unique()\n",
    "    ids_splits = np.array_split(ids, 4)\n",
    "\n",
    "    results = Parallel(n_jobs=4, backend=\"multiprocessing\")(\n",
    "        delayed(_prepare_test_data_helper)(tokenizer, idx) for idx in ids_splits\n",
    "    )\n",
    "    for result in results:\n",
    "        test_samples.extend(result)\n",
    "\n",
    "    return test_samples\n",
    "\n",
    "\n",
    "class FeedbackDataset:\n",
    "    def __init__(self, samples, max_len, tokenizer):\n",
    "        self.samples = samples\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "        self.length = len(samples)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.samples[idx][\"input_ids\"]\n",
    "        input_id = self.samples[idx][\"id\"]\n",
    "        input_text = self.samples[idx][\"text\"]\n",
    "        input_offset = self.samples[idx][\"offset_mapping\"]\n",
    "        \n",
    "\n",
    "        # add start token id to the input_ids\n",
    "        input_ids = [self.tokenizer.cls_token_id] + input_ids\n",
    "\n",
    "        if len(input_ids) > self.max_len - 1:\n",
    "            input_ids = input_ids[: self.max_len - 1]\n",
    "\n",
    "        # add end token id to the input_ids\n",
    "        input_ids = input_ids + [self.tokenizer.sep_token_id]\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "\n",
    "        return {\n",
    "            \"id\":input_id,\n",
    "            'text':input_text,\n",
    "            \"token_id\": input_ids,\n",
    "            \"token_mask\": attention_mask,\n",
    "            \"token_offset\":str(input_offset),\n",
    "        }\n",
    "\n",
    "class Collate:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        output = dict()\n",
    "        output[\"id\"] = [sample[\"id\"] for sample in batch]\n",
    "        output[\"token_offset\"] = [sample[\"token_offset\"] for sample in batch]\n",
    "        output[\"text\"] = [sample[\"text\"] for sample in batch]\n",
    "        \n",
    "        output[\"token_id\"] = [sample[\"token_id\"] for sample in batch]\n",
    "        output[\"token_mask\"] = [sample[\"token_mask\"] for sample in batch]\n",
    "\n",
    "        # calculate max token length of this batch\n",
    "        batch_max = max([len(token_id) for token_id in output[\"token_id\"]])\n",
    "\n",
    "        # add padding\n",
    "        if self.tokenizer.padding_side == \"right\":\n",
    "            output[\"token_id\"] = [s + (batch_max - len(s)) * [self.tokenizer.pad_token_id] for s in output[\"token_id\"]]\n",
    "            output[\"token_mask\"] = [s + (batch_max - len(s)) * [0] for s in output[\"token_mask\"]]\n",
    "        else:\n",
    "            output[\"token_id\"] = [(batch_max - len(s)) * [self.tokenizer.pad_token_id] + s for s in output[\"token_id\"]]\n",
    "            output[\"token_mask\"] = [(batch_max - len(s)) * [0] + s for s in output[\"token_mask\"]]\n",
    "\n",
    "        # convert to tensors\n",
    "        output[\"token_id\"] = torch.tensor(output[\"token_id\"], dtype=torch.long)\n",
    "        output[\"token_mask\"] = torch.tensor(output[\"token_mask\"], dtype=torch.long)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec0d452f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T18:50:07.827940Z",
     "iopub.status.busy": "2022-03-15T18:50:07.826988Z",
     "iopub.status.idle": "2022-03-15T18:50:07.828883Z",
     "shell.execute_reply": "2022-03-15T18:50:07.829360Z",
     "shell.execute_reply.started": "2022-03-15T18:43:47.823112Z"
    },
    "papermill": {
     "duration": 0.02529,
     "end_time": "2022-03-15T18:50:07.829513",
     "exception": false,
     "start_time": "2022-03-15T18:50:07.804223",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FeedbackModel(tez.Model):\n",
    "    def __init__(self, model_name, num_labels=num_discourse_marker):\n",
    "        super().__init__()\n",
    "        self.model_name = model_name\n",
    "        self.num_labels = num_labels\n",
    "        config = AutoConfig.from_pretrained(model_name)\n",
    "\n",
    "        hidden_dropout_prob: float = 0.1\n",
    "        layer_norm_eps: float = 1e-7\n",
    "        config.update(\n",
    "            {\n",
    "                \"output_hidden_states\": True,\n",
    "                \"hidden_dropout_prob\": hidden_dropout_prob,\n",
    "                \"layer_norm_eps\": layer_norm_eps,\n",
    "                \"add_pooling_layer\": False,\n",
    "            }\n",
    "        )\n",
    "        self.transformer = AutoModel.from_config(config)\n",
    "        self.output = nn.Linear(config.hidden_size, self.num_labels)\n",
    "\n",
    "    def forward(self, ids, mask):\n",
    "        transformer_out = self.transformer(ids, mask)\n",
    "        sequence_output = transformer_out.last_hidden_state\n",
    "        logits = self.output(sequence_output)\n",
    "        logits = torch.softmax(logits, dim=-1)\n",
    "        return logits, 0, {}\n",
    "\n",
    "\n",
    "checkpoint =[   \n",
    "    [\n",
    "     '../input/feedbackdeberta/model/model_0.bin',\n",
    "     '../input/feedbackdeberta/model/model_1.bin',\n",
    "     '../input/feedbackdeberta/model/model_2.bin',\n",
    "     '../input/feedbackdeberta/model/model_3.bin',\n",
    "     '../input/feedbackdeberta/model/model_4.bin',\n",
    "    ],\n",
    "    [\n",
    "     '../input/mybirdsmodel/dfe1.bin',\n",
    "     '../input/mybirdsmodel/dfe5.bin',\n",
    "    ],\n",
    "    [\n",
    "    '../input/mybirdsmodel/dx0.bin',\n",
    "    '../input/mybirdsmodel/dx1.bin',\n",
    "    ]\n",
    "]\n",
    "\n",
    "net_type = [\n",
    "    \n",
    "    [FeedbackModel,   '../input/debertalarge/' ],\n",
    "    [FeedbackModel,'../input/debertalarge/'],\n",
    "    [FeedbackModel,   '../input/deberta-xlarge/' ],\n",
    "]\n",
    "\n",
    "num_net = sum([len(i) for i in checkpoint])\n",
    "num_net1 = sum([len(i) for i in checkpoint[:1]])\n",
    "num_net2 = sum([len(i) for i in checkpoint[:2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dedfe82b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T18:50:07.870974Z",
     "iopub.status.busy": "2022-03-15T18:50:07.860334Z",
     "iopub.status.idle": "2022-03-15T18:50:07.892031Z",
     "shell.execute_reply": "2022-03-15T18:50:07.891491Z",
     "shell.execute_reply.started": "2022-03-15T18:43:47.837667Z"
    },
    "papermill": {
     "duration": 0.051662,
     "end_time": "2022-03-15T18:50:07.892177",
     "exception": false,
     "start_time": "2022-03-15T18:50:07.840515",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#processing\n",
    "\n",
    "def text_to_word(text):\n",
    "    word = text.split()\n",
    "    word_offset = []\n",
    "\n",
    "    start = 0\n",
    "    for w in word:\n",
    "        r = text[start:].find(w)\n",
    "\n",
    "        if r==-1:\n",
    "            raise NotImplementedError\n",
    "        else:\n",
    "            start = start+r\n",
    "            end   = start+len(w)\n",
    "            word_offset.append((start,end))\n",
    "            #print('%32s'%w, '%5d'%start, '%5d'%r, text[start:end])\n",
    "        start = end\n",
    "\n",
    "    return word, word_offset\n",
    "\n",
    "def word_probability_to_predict_df(text_to_word_probability, id):\n",
    "    len_word = len(text_to_word_probability)\n",
    "    word_predict = text_to_word_probability.argmax(-1)\n",
    "    word_score   = text_to_word_probability.max(-1)\n",
    "    predict_df = []\n",
    "\n",
    "    t = 0\n",
    "    while 1:\n",
    "        if word_predict[t] not in [\n",
    "            discourse_marker_to_label['O'],\n",
    "            discourse_marker_to_label['PAD'],\n",
    "        ]:\n",
    "            start = t\n",
    "            b_marker_label = word_predict[t]\n",
    "        else:\n",
    "            t = t+1\n",
    "            if t== len_word-1: break\n",
    "            continue\n",
    "\n",
    "        t = t+1\n",
    "        if t== len_word-1: break\n",
    "\n",
    "        #----\n",
    "        if   label_to_discourse_marker[b_marker_label][0]=='B':\n",
    "            i_marker_label = b_marker_label+1\n",
    "        elif label_to_discourse_marker[b_marker_label][0]=='I':\n",
    "            i_marker_label = b_marker_label\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        while 1:\n",
    "            #print(t)\n",
    "            if (word_predict[t] != i_marker_label) or (t ==len_word-1):\n",
    "                end = t\n",
    "                # prediction_string = ' '.join([str(i) for i in range(start,end)]) #np.arange(start,end).tolist()\n",
    "                discourse_type = label_to_discourse_marker[b_marker_label][2:]\n",
    "                discourse_score = word_score[start:end].tolist()\n",
    "                # predict_df.append((id, discourse_type, prediction_string, str(discourse_score)))\n",
    "                predict_df.append((id, start, end, discourse_type, str(discourse_score)))\n",
    "                break\n",
    "            else:\n",
    "                t = t+1\n",
    "                continue\n",
    "        if t== len_word-1: break\n",
    "    \n",
    "    temp_df = []\n",
    "    for phrase_idx, (_, word_start, word_end, label, str_scores) in enumerate(predict_df):\n",
    "        if label == 'Lead':\n",
    "            word_end = min(word_end, int(len_word * 0.28))\n",
    "            if word_start > int(len_word * 0.20):\n",
    "                continue\n",
    "        elif label == 'Concluding Statement':\n",
    "            word_start = max(word_start, int(len_word * 0.67))\n",
    "            if word_end < int(len_word * 0.90):\n",
    "                continue\n",
    "        if word_end < word_start:\n",
    "            continue\n",
    "        if label == 'Rebuttal' and len(temp_df) < 1:\n",
    "            continue\n",
    "        if label == 'Rebuttal':\n",
    "            word_end = min(word_end, word_start + 45)\n",
    "        if label == 'Counterclaim':\n",
    "            word_end = min(word_end, word_start + 45)\n",
    "        if label == 'Position':\n",
    "            word_end = min(word_end, word_start + 40)\n",
    "        if label == 'Claim':\n",
    "            word_end = min(word_end, word_start + 31)\n",
    "        ps = \" \".join([str(x) for x in range(word_start, word_end)])\n",
    "        temp_df.append((id, label, ps, str_scores)) \n",
    "    predict_df = pd.DataFrame(temp_df, columns=['id', 'class', 'predictionstring', 'score'])\n",
    "    return predict_df\n",
    "\n",
    "def do_threshold(submit_df, use=['length','probability']):\n",
    "    df = submit_df.copy()\n",
    "    df = df.fillna('')\n",
    "\n",
    "    if 'length' in use:\n",
    "        df['l'] = df.predictionstring.apply(lambda x: len(x.split()))\n",
    "        for key, value in length_threshold.items():\n",
    "            #value=3\n",
    "            index = df.loc[df['class'] == key].query('l<%d'%value).index\n",
    "            df.drop(index, inplace=True)\n",
    "\n",
    "    if 'probability' in use:\n",
    "        df['s'] = df.score.apply(lambda x: np.mean(eval(x)))\n",
    "        for key, value in probability_threshold.items():\n",
    "            index = df.loc[df['class'] == key].query('s<%f'%value).index\n",
    "            df.drop(index, inplace=True)\n",
    "\n",
    "    df = df[['id', 'class', 'predictionstring']]\n",
    "    return df\n",
    "\n",
    "#evaluation for debug ----\n",
    "# https://www.kaggle.com/cpmpml/faster-metric-computation\n",
    "\n",
    "def compute_overlap(predict, truth):\n",
    "    \"\"\"\n",
    "    Calculates the overlap between prediction and\n",
    "    ground truth and overlap percentages used for determining\n",
    "    true positives.\n",
    "    \"\"\"\n",
    "    # Length of each and intersection\n",
    "    try:\n",
    "        len_truth   = len(truth)\n",
    "        len_predict = len(predict)\n",
    "        intersect = len(truth & predict)\n",
    "        overlap1 = intersect/ len_truth\n",
    "        overlap2 = intersect/ len_predict\n",
    "        return (overlap1, overlap2)\n",
    "    except:  # at least one of the input is NaN\n",
    "        return (0, 0)\n",
    "\n",
    "def compute_f1_score_one(predict_df, truth_df, discourse_type):\n",
    "    \"\"\"\n",
    "    A function that scores for the kaggle\n",
    "        Student Writing Competition\n",
    "\n",
    "    Uses the steps in the evaluation page here:\n",
    "        https://www.kaggle.com/c/feedback-prize-2021/overview/evaluation\n",
    "    \"\"\"\n",
    "    t_df = truth_df.loc[truth_df['discourse_type'] == discourse_type,   ['id', 'predictionstring']].reset_index(drop=True)\n",
    "    p_df = predict_df.loc[predict_df['class'] == discourse_type,  ['id', 'predictionstring']].reset_index(drop=True)\n",
    "\n",
    "    p_df.loc[:,'predict_id'] = p_df.index\n",
    "    t_df.loc[:,'truth_id'] = t_df.index\n",
    "    p_df.loc[:,'predictionstring'] = [set(p.split(' ')) for p in p_df['predictionstring']]\n",
    "    t_df.loc[:,'predictionstring'] = [set(p.split(' ')) for p in t_df['predictionstring']]\n",
    "\n",
    "    # Step 1. all ground truths and predictions for a given class are compared.\n",
    "    joined = p_df.merge(t_df,\n",
    "                           left_on='id',\n",
    "                           right_on='id',\n",
    "                           how='outer',\n",
    "                           suffixes=('_p','_t')\n",
    "                          )\n",
    "    overlap = [compute_overlap(*predictionstring) for predictionstring in zip(joined.predictionstring_p, joined.predictionstring_t)]\n",
    "\n",
    "    # 2. If the overlap between the ground truth and prediction is >= 0.5,\n",
    "    # and the overlap between the prediction and the ground truth >= 0.5,\n",
    "    # the prediction is a match and considered a true positive.\n",
    "    # If multiple matches exist, the match with the highest pair of overlaps is taken.\n",
    "    joined['potential_TP'] = [(o[0] >= 0.5 and o[1] >= 0.5) for o in overlap]\n",
    "    joined['max_overlap' ] = [max(*o) for o in overlap]\n",
    "    joined_tp = joined.query('potential_TP').reset_index(drop=True)\n",
    "    tp_pred_ids = joined_tp\\\n",
    "        .sort_values('max_overlap', ascending=False) \\\n",
    "        .groupby(['id','truth_id'])['predict_id'].first()\n",
    "\n",
    "    # 3. Any unmatched ground truths are false negatives\n",
    "    # and any unmatched predictions are false positives.\n",
    "    fp_pred_ids = set(joined['predict_id'].unique()) - set(tp_pred_ids)\n",
    "\n",
    "    matched_gt_ids   = joined_tp['truth_id'].unique()\n",
    "    unmatched_gt_ids = set(joined['truth_id'].unique()) -  set(matched_gt_ids)\n",
    "\n",
    "    # Get numbers of each type\n",
    "    TP = len(tp_pred_ids)\n",
    "    FP = len(fp_pred_ids)\n",
    "    FN = len(unmatched_gt_ids)\n",
    "    f1 = TP / (TP + 0.5*(FP+FN))\n",
    "    return f1\n",
    "\n",
    "def compute_lb_f1_score(predict_df, truth_df):\n",
    "    f1_score = {}\n",
    "    for discourse_type in truth_df.discourse_type.unique():\n",
    "        f1_score[discourse_type] = compute_f1_score_one(predict_df, truth_df, discourse_type)\n",
    "    #f1 = np.mean([v for v in class_scores.values()])\n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df591151",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T18:50:07.940972Z",
     "iopub.status.busy": "2022-03-15T18:50:07.917030Z",
     "iopub.status.idle": "2022-03-15T18:50:07.943480Z",
     "shell.execute_reply": "2022-03-15T18:50:07.942967Z",
     "shell.execute_reply.started": "2022-03-15T18:43:47.873767Z"
    },
    "papermill": {
     "duration": 0.040606,
     "end_time": "2022-03-15T18:50:07.943611",
     "exception": false,
     "start_time": "2022-03-15T18:50:07.903005",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def jn(pst, start, end):\n",
    "    pst_temp = pst[start: end]\n",
    "    while -1 in pst_temp:\n",
    "        pst_temp.remove(-1)\n",
    "    return \" \".join([str(x) for x in pst_temp])\n",
    "\n",
    "\n",
    "def link_evidence(oof):\n",
    "    thresh = 1\n",
    "    idu = oof['id'].unique()\n",
    "    idc = idu[1]\n",
    "    eoof = oof[oof['class'] == \"Evidence\"]\n",
    "    neoof = oof[oof['class'] != \"Evidence\"]\n",
    "    for thresh2 in range(26, 27, 1): # 26 27 1\n",
    "        retval = []\n",
    "        for idv in idu:\n",
    "            for c in  ['Lead', 'Position', 'Evidence', 'Claim', 'Concluding Statement',\n",
    "                   'Counterclaim', 'Rebuttal']:\n",
    "                q = eoof[(eoof['id'] == idv) & (eoof['class'] == c)]\n",
    "                if len(q) == 0:\n",
    "                    continue\n",
    "                pst = []\n",
    "                for i,r in q.iterrows():\n",
    "                    pst = pst +[-1] + [int(x) for x in r['predictionstring'].split()]\n",
    "                start = 1\n",
    "                end = 1\n",
    "                for i in range(2,len(pst)):\n",
    "                    cur = pst[i]\n",
    "                    end = i\n",
    "                    #if pst[start] == 205:\n",
    "                    #   print(cur, pst[start], cur - pst[start])\n",
    "                    if (cur == -1 and c != 'Evidence') or ((cur == -1) and ((pst[i+1] > pst[end-1] + thresh) or (pst[i+1] - pst[start] > thresh2))):\n",
    "                        retval.append((idv, c, jn(pst, start, end)))\n",
    "                        start = i + 1\n",
    "                v = (idv, c, jn(pst, start, end+1))\n",
    "                #print(v)\n",
    "                retval.append(v)\n",
    "        roof = pd.DataFrame(retval, columns = ['id', 'class', 'predictionstring']) \n",
    "        roof = roof.merge(neoof, how='outer')\n",
    "        return roof\n",
    "    \n",
    "\n",
    "def link_others(oof, c):\n",
    "    idu = oof['id'].unique()\n",
    "    idc = idu[1]\n",
    "    eoof = oof[oof['class'] == c]\n",
    "    neoof = oof[oof['class'] != c]\n",
    "    retval = []\n",
    "    for idv in idu:\n",
    "        q = eoof[(eoof['id'] == idv) & (eoof['class'] == c)]\n",
    "        if len(q) == 0:\n",
    "            continue\n",
    "        pst = []\n",
    "        for i,r in q.iterrows():\n",
    "            pst = pst +[-1] + [int(x) for x in r['predictionstring'].split()]\n",
    "        start = 1\n",
    "        end = 1\n",
    "        for i in range(2, len(pst)):\n",
    "            cur = pst[i]\n",
    "            end = i\n",
    "            if (cur == -1) and (pst[i+1] > pst[end-1] + 1):\n",
    "                retval.append((idv, c, jn(pst, start, end)))\n",
    "                start = i + 1\n",
    "        v = (idv, c, jn(pst, start, end+1))\n",
    "        retval.append(v)\n",
    "    roof = pd.DataFrame(retval, columns = ['id', 'class', 'predictionstring']) \n",
    "    roof = roof.merge(neoof, how='outer')\n",
    "    return roof\n",
    "\n",
    "\n",
    "def is_in2(s, text):\n",
    "    s_len = len(s)\n",
    "    l = 0; r = s_len\n",
    "    while r < len(text):\n",
    "        if text[l: r] == s:\n",
    "            return True\n",
    "        l += 1; r += 1\n",
    "    return False\n",
    "\n",
    "\n",
    "def change_rebuttal(df):\n",
    "    temp = df.reset_index(drop=True)\n",
    "    for i in range(1, len(temp)):\n",
    "        if temp.loc[i - 1, \"class\"] in ['Counterclaim'] and temp.loc[i, \"class\"] != 'Rebuttal':\n",
    "            text_id = temp.loc[i, \"id\"] # predictionstring\n",
    "            word_nums = [int(x) for x in temp.loc[i, \"predictionstring\"].split()]\n",
    "            word_start = word_nums[0]\n",
    "            with open('../input/feedback-prize-2021/test/' + str(text_id) + '.txt', 'r') as f: # test\n",
    "                text = f.read()\n",
    "            text = text.replace(u'\\xa0', u' ')\n",
    "            text = text.rstrip()\n",
    "            text = text.lstrip()\n",
    "            text = text.split()\n",
    "            if is_in2('owever', ' '.join(text[word_start: word_start + 1])):\n",
    "                print('Change!', temp.loc[i, \"class\"])\n",
    "                temp.loc[i, \"class\"] = 'Rebuttal'\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db8bcde8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-15T18:50:08.000068Z",
     "iopub.status.busy": "2022-03-15T18:50:07.969152Z",
     "iopub.status.idle": "2022-03-15T18:53:39.844968Z",
     "shell.execute_reply": "2022-03-15T18:53:39.845405Z",
     "shell.execute_reply.started": "2022-03-15T18:43:47.903946Z"
    },
    "papermill": {
     "duration": 211.890851,
     "end_time": "2022-03-15T18:53:39.845577",
     "exception": false,
     "start_time": "2022-03-15T18:50:07.954726",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start ram memory gb :0.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (798 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (755 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1302 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load ok : [0] ../input/debertalarge/\n",
      "              ../input/feedbackdeberta/model/model_0.bin\n",
      "after allocate net 0 ram memory gb :1.73\n",
      "\t5/5   0 min 01 sec\n",
      "after gc.collect() ram memory gb :1.95\n",
      "\n",
      "load ok : [1] ../input/debertalarge/\n",
      "              ../input/feedbackdeberta/model/model_1.bin\n",
      "after allocate net 1 ram memory gb :1.96\n",
      "\t5/5   0 min 01 sec\n",
      "after gc.collect() ram memory gb :1.96\n",
      "\n",
      "load ok : [2] ../input/debertalarge/\n",
      "              ../input/feedbackdeberta/model/model_2.bin\n",
      "after allocate net 2 ram memory gb :1.96\n",
      "\t5/5   0 min 01 sec\n",
      "after gc.collect() ram memory gb :1.96\n",
      "\n",
      "load ok : [3] ../input/debertalarge/\n",
      "              ../input/feedbackdeberta/model/model_3.bin\n",
      "after allocate net 3 ram memory gb :1.96\n",
      "\t5/5   0 min 01 sec\n",
      "after gc.collect() ram memory gb :1.96\n",
      "\n",
      "load ok : [4] ../input/debertalarge/\n",
      "              ../input/feedbackdeberta/model/model_4.bin\n",
      "after allocate net 4 ram memory gb :1.96\n",
      "\t5/5   0 min 01 sec\n",
      "after gc.collect() ram memory gb :1.96\n",
      "\n",
      "after gc.collect() ram memory gb :1.96\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (798 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (755 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1302 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load ok : [0] ../input/debertalarge/\n",
      "              ../input/mybirdsmodel/dfe1.bin\n",
      "after allocate net 0 ram memory gb :3.3\n",
      "\t5/5   0 min 01 sec\n",
      "after gc.collect() ram memory gb :3.3\n",
      "\n",
      "load ok : [1] ../input/debertalarge/\n",
      "              ../input/mybirdsmodel/dfe5.bin\n",
      "after allocate net 1 ram memory gb :3.3\n",
      "\t5/5   0 min 01 sec\n",
      "after gc.collect() ram memory gb :3.3\n",
      "\n",
      "after gc.collect() ram memory gb :3.3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (798 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (755 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1302 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load ok : [0] ../input/deberta-xlarge/\n",
      "              ../input/mybirdsmodel/dx0.bin\n",
      "after allocate net 0 ram memory gb :4.86\n",
      "\t5/5   0 min 02 sec\n",
      "after gc.collect() ram memory gb :4.86\n",
      "\n",
      "load ok : [1] ../input/deberta-xlarge/\n",
      "              ../input/mybirdsmodel/dx1.bin\n",
      "after allocate net 1 ram memory gb :4.65\n",
      "\t5/5   0 min 02 sec\n",
      "after gc.collect() ram memory gb :4.65\n",
      "\n",
      "after gc.collect() ram memory gb :4.65\n",
      "\n",
      "\n",
      "----\n",
      "             id                 class  \\\n",
      "0  D46BCB48440A  Concluding Statement   \n",
      "1  D72CB1C11673  Concluding Statement   \n",
      "2  DF920E0A7337  Concluding Statement   \n",
      "3  18409261F5C2  Concluding Statement   \n",
      "4  0FB0700DAF44  Concluding Statement   \n",
      "\n",
      "                                    predictionstring  \n",
      "0  306 307 308 309 310 311 312 313 314 315 316 31...  \n",
      "1  364 365 366 367 368 369 370 371 372 373 374 37...  \n",
      "2  620 621 622 623 624 625 626 627 628 629 630 63...  \n",
      "3  989 990 991 992 993 994 995 996 997 998 999 10...  \n",
      "4  560 561 562 563 564 565 566 567 568 569 570 57...  \n",
      "submission ok!----\n"
     ]
    }
   ],
   "source": [
    "def memory_used_to_str():\n",
    "    pid = os.getpid()\n",
    "    processs = psutil.Process(pid)\n",
    "    memory_use = processs.memory_info()[0] / 2. ** 30\n",
    "    return 'ram memory gb :' + str(np.round(memory_use, 2))\n",
    "if 1:\n",
    "    print('start', memory_used_to_str())\n",
    "##############################################################\n",
    "\n",
    "def run_submit():\n",
    "    if is_debug: print(\"THIS IS DEBUG ####################################\")\n",
    "    results = []\n",
    "    \n",
    "    for net_type_num in range(3):\n",
    "        Net, arch = net_type[net_type_num]\n",
    "        net = Net(arch)\n",
    "        if 'v3' in arch:\n",
    "            from transformers.models.deberta_v2 import DebertaV2TokenizerFast\n",
    "            tokenizer = DebertaV2TokenizerFast.from_pretrained(arch)\n",
    "        else:\n",
    "            tokenizer = AutoTokenizer.from_pretrained(arch)\n",
    "\n",
    "        test_samples = prepare_test_data(df_text, tokenizer)\n",
    "        collate = Collate(tokenizer=tokenizer)\n",
    "        valid_dataset = FeedbackDataset(test_samples, max_length, tokenizer)\n",
    "        valid_loader  = DataLoader(\n",
    "            valid_dataset,\n",
    "            sampler = SequentialSampler(valid_dataset),\n",
    "            batch_size  = 2, # 8\n",
    "            drop_last   = False,\n",
    "            num_workers = 2, \n",
    "            pin_memory  = False,\n",
    "            collate_fn = collate,\n",
    "        )    \n",
    "        ######### checkpoint\n",
    "        for n in range(len(checkpoint[net_type_num])):\n",
    "            net.load(checkpoint[net_type_num][n], weights_only=True)\n",
    "\n",
    "            if is_cuda:\n",
    "                net.cuda()\n",
    "            print('load ok : [%d] %s'%(n, arch))\n",
    "            print('              %s'%(checkpoint[net_type_num][n]))\n",
    "            print('after allocate net %d'%n, memory_used_to_str())\n",
    "            results_n = {\n",
    "                'id':[],\n",
    "                'token_mask':[],\n",
    "                'token_offset':[],\n",
    "                'probability':[],\n",
    "            }\n",
    "\n",
    "            T = 0\n",
    "            start_timer = timer()\n",
    "            for t, batch in enumerate(valid_loader):\n",
    "                batch_size = len(batch['id'])\n",
    "                token_mask = batch['token_mask']\n",
    "                token_id   = batch['token_id']\n",
    "                if is_cuda:\n",
    "                    token_mask = token_mask.cuda()\n",
    "                    token_id = token_id.cuda()\n",
    "\n",
    "                net.eval()\n",
    "                with torch.no_grad():\n",
    "                    with amp.autocast(enabled=is_amp):\n",
    "\n",
    "                        probability = data_parallel(net,(token_id, token_mask))\n",
    "                        # probability = net[n](token_id, token_mask)\n",
    "                        pp = (probability[0] * 255).byte().data.cpu().numpy()\n",
    "                        if pp.shape[1] > max_length:\n",
    "                            pp = pp[:, :max_length, :]\n",
    "                        else:\n",
    "                            pp = np.pad(pp,((0, 0), (0, max_length - pp.shape[1]), (0, 0)),'constant', constant_values=0) \n",
    "                        #probability = 1\n",
    "                        #pp = np.random.randint(0,255,size=[len(batch['token_offset']), max_length, 15]).astype('int8')\n",
    "                        results_n['probability'].append( pp )\n",
    "                        if n == 0:\n",
    "                            results_n['token_offset' ] += [eval(x) for x in batch['token_offset']]\n",
    "                        T += batch_size\n",
    "\n",
    "                print('\\r\\t%d/%d  %s'%(T, len(valid_dataset), time_to_str(timer() - start_timer,'sec')),end='',flush=True)\n",
    "\n",
    "            #----------------------------\n",
    "            if is_cuda: torch.cuda.empty_cache()\n",
    "            print('')\n",
    "            if n == 0:\n",
    "                results.append({\n",
    "                    'probability' : np.concatenate(results_n['probability']),\n",
    "                    'token_offset': np.array(results_n['token_offset'], object)\n",
    "                })\n",
    "            else:\n",
    "                 results.append({\n",
    "                    'probability' : np.concatenate(results_n['probability']),\n",
    "                })           \n",
    "\n",
    "            del probability, pp, results_n\n",
    "            gc.collect()\n",
    "            print('after gc.collect()', memory_used_to_str())\n",
    "            print()\n",
    "        #------------------------------------------------------------------------\n",
    "        del net, test_samples, Net, tokenizer\n",
    "        gc.collect()\n",
    "        print('after gc.collect()', memory_used_to_str())\n",
    "        print()   \n",
    "        ##############################################################\n",
    "\n",
    "    \n",
    "    ##### concat\n",
    "    submit_df = []\n",
    "    for i in range(num_valid):\n",
    "        d  = df_text.iloc[i]\n",
    "        id = d.id\n",
    "        text = d.text\n",
    "        word, word_offset = text_to_word(text)\n",
    "        token_to_text_probability = np.full((len(text),num_discourse_marker),0, np.float32)\n",
    "        for j in range(num_net):\n",
    "            p = results[j]['probability'][i][1:]/255  \n",
    "            if j < num_net1:\n",
    "                for t,(start,end) in enumerate(results[0]['token_offset'][i]):\n",
    "                    if t==max_length-1: break #assume max_length, else use token_mask to get length\n",
    "                    token_to_text_probability[start:end]+=p[t] #**0.5     \n",
    "            elif j < num_net2:\n",
    "                for t,(start,end) in enumerate(results[num_net1]['token_offset'][i]):\n",
    "                    if t==max_length-1: break #assume max_length, else use token_mask to get length\n",
    "                    token_to_text_probability[start:end]+=p[t] #**0.5  \n",
    "            else:\n",
    "                for t,(start,end) in enumerate(results[num_net2]['token_offset'][i]):\n",
    "                    if t==max_length-1: break #assume max_length, else use token_mask to get length\n",
    "                    token_to_text_probability[start:end]+=p[t] #**0.5  \n",
    "            \n",
    "        token_to_text_probability = token_to_text_probability/num_net\n",
    "        \n",
    "        text_to_word_probability = np.full((len(word),num_discourse_marker),0, np.float32)\n",
    "        for t,(start,end) in enumerate(word_offset):\n",
    "            text_to_word_probability[t]=token_to_text_probability[start:end].mean(0)\n",
    "\n",
    "        predict_df = word_probability_to_predict_df(text_to_word_probability, id)\n",
    "        submit_df.append(predict_df)\n",
    "    print('')\n",
    "\n",
    "    #----------------------------------------\n",
    "    submit_df = pd.concat(submit_df).reset_index(drop=True) \n",
    "    submit_df = do_threshold(submit_df, use=['length', 'probability'])\n",
    "    \n",
    "    submit_df = change_rebuttal(submit_df)\n",
    "    submit_df = link_evidence(submit_df)\n",
    "    for c in  ['Lead', 'Position', 'Concluding Statement', 'Counterclaim', 'Rebuttal']:\n",
    "        submit_df = link_others(submit_df, c)\n",
    "    \n",
    "    submit_df.to_csv('submission.csv', index=False)\n",
    "    print('----')\n",
    "    print(submit_df.head())\n",
    "    print('submission ok!----')\n",
    "    if is_debug:\n",
    "        f1_score = compute_lb_f1_score(submit_df, valid_df)\n",
    "        print('f1 macro : %f\\n' % np.mean([v for v in f1_score.values()]))\n",
    "        for k,v in f1_score.items():\n",
    "            print('%20s : %05f'%(k,v))\n",
    "            \n",
    "run_submit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 226.519137,
   "end_time": "2022-03-15T18:53:42.905806",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-03-15T18:49:56.386669",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
